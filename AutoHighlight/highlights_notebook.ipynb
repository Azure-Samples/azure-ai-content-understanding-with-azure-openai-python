{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c7c57c",
   "metadata": {},
   "source": [
    "# Video Highlights Generation\n",
    "\n",
    "This notebook generates video highlights from a video file using Azure AI services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa68553",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "1. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4810bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import importlib.util\n",
    "\n",
    "# Complete package list for AutoHighlight\n",
    "packages = [\n",
    "    # Core dependencies\n",
    "    'openai>=1.3.3', \n",
    "    'python-dotenv',\n",
    "    'requests',\n",
    "    'jsonschema',\n",
    "    \n",
    "    # Azure AI services\n",
    "    'azure-ai-vision-imageanalysis==1.0.0b3',  # Correct Azure Vision package\n",
    "    'azure-identity',\n",
    "    'azure-core',\n",
    "    \n",
    "    # Video processing (FFmpeg-based)\n",
    "    'ffmpeg-python',\n",
    "    'Pillow>=10.0.0',\n",
    "    \n",
    "    # Scientific computing\n",
    "    'numpy>=1.23.5',\n",
    "    'scipy>=1.10',\n",
    "    \n",
    "    # Optional: Audio processing (if needed for advanced features)\n",
    "    'librosa==0.10.1.post2',\n",
    "    'soundfile>=0.12.1',\n",
    "]\n",
    "\n",
    "def check_package_installed(package_name):\n",
    "    \"\"\"Check if a package is already installed\"\"\"\n",
    "    spec_name = package_name.split('>=')[0].split('==')[0].split('<')[0]\n",
    "    return importlib.util.find_spec(spec_name) is not None\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a single package with error handling\"\"\"\n",
    "    try:\n",
    "        # Check if already installed\n",
    "        package_name = package.split('>=')[0].split('==')[0].split('<')[0]\n",
    "        if check_package_installed(package_name):\n",
    "            print(f\"âœ… {package_name} already installed\")\n",
    "            return True\n",
    "            \n",
    "        # Install the package\n",
    "        print(f\"ğŸ“¦ Installing {package}...\")\n",
    "        result = subprocess.run([sys.executable, '-m', 'pip', 'install', package], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… Successfully installed {package}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ Failed to install {package}: {result.stderr}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error installing {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"ğŸš€ Installing AutoHighlight dependencies...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Install packages\n",
    "failed_packages = []\n",
    "for package in packages:\n",
    "    if not install_package(package):\n",
    "        failed_packages.append(package)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ğŸ“‹ Installation Summary:\")\n",
    "print(f\"âœ… Successfully installed: {len(packages) - len(failed_packages)}/{len(packages)} packages\")\n",
    "\n",
    "if failed_packages:\n",
    "    print(f\"âŒ Failed packages: {len(failed_packages)}\")\n",
    "    for pkg in failed_packages:\n",
    "        print(f\"   - {pkg}\")\n",
    "    print(\"\\nğŸ’¡ You may need to install these manually or check your internet connection\")\n",
    "else:\n",
    "    print(\"ğŸ‰ All packages installed successfully!\")\n",
    "\n",
    "# Check critical dependencies\n",
    "print(\"\\nğŸ” Verifying critical imports...\")\n",
    "critical_imports = [\n",
    "    ('openai', 'OpenAI API client'),\n",
    "    ('azure.ai.vision.imageanalysis', 'Azure AI Vision'),\n",
    "    ('ffmpeg', 'FFmpeg Python wrapper'),\n",
    "    ('numpy', 'NumPy for numerical computing'),\n",
    "    ('PIL', 'Pillow for image processing')\n",
    "]\n",
    "\n",
    "import_results = []\n",
    "for module, description in critical_imports:\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"âœ… {description}\")\n",
    "        import_results.append(True)\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {description} - Import failed\")\n",
    "        import_results.append(False)\n",
    "\n",
    "# FFmpeg system check\n",
    "print(\"\\nğŸ¬ Checking FFmpeg availability...\")\n",
    "try:\n",
    "    result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True, timeout=5)\n",
    "    if result.returncode == 0:\n",
    "        version_line = result.stdout.split('\\n')[0]\n",
    "        print(f\"âœ… FFmpeg found: {version_line}\")\n",
    "    else:\n",
    "        print(\"âŒ FFmpeg command failed\")\n",
    "        print(\"ğŸ’¡ Please install FFmpeg: https://ffmpeg.org/download.html\")\n",
    "except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "    print(\"âŒ FFmpeg not found in system PATH\")\n",
    "    print(\"ğŸ’¡ Install FFmpeg:\")\n",
    "    print(\"   - Windows: winget install ffmpeg\")\n",
    "    print(\"   - Or download from: https://ffmpeg.org/download.html\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Pipeline readiness: {'READY' if all(import_results) else 'NEEDS ATTENTION'}\")\n",
    "print(\"Package installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99419176",
   "metadata": {},
   "source": [
    "## API Configuration\n",
    "\n",
    "**New users:** Before running the pipeline, you need to configure your Azure API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1406d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Services (for video analysis)\n",
    "AZURE_AI_ENDPOINT = \"\"\n",
    "\n",
    "# Azure OpenAI (for highlight reasoning)\n",
    "AZURE_OPENAI_ENDPOINT = \"\"\n",
    "AZURE_OPENAI_API_KEY = \"\"\n",
    "\n",
    "# Validate that credentials have been updated\n",
    "if \"your-ai-service\" in AZURE_AI_ENDPOINT:\n",
    "    raise ValueError(\"âš ï¸ Please update AZURE_AI_ENDPOINT with your actual Azure AI service endpoint!\")\n",
    "\n",
    "if \"your-openai-service\" in AZURE_OPENAI_ENDPOINT:\n",
    "    raise ValueError(\"âš ï¸ Please update AZURE_OPENAI_ENDPOINT with your actual Azure OpenAI endpoint!\")\n",
    "\n",
    "if AZURE_OPENAI_API_KEY == \"your_openai_api_key_here\":\n",
    "    raise ValueError(\"âš ï¸ Please update AZURE_OPENAI_API_KEY with your actual API key!\")\n",
    "\n",
    "# Set environment variables for other modules to use\n",
    "import os\n",
    "os.environ[\"AZURE_AI_ENDPOINT\"] = AZURE_AI_ENDPOINT\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_OPENAI_ENDPOINT  \n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = AZURE_OPENAI_API_KEY\n",
    "\n",
    "print(\"âœ… API credentials configured successfully!\")\n",
    "print(f\"Azure AI Endpoint: {AZURE_AI_ENDPOINT}\")\n",
    "print(f\"Azure OpenAI Endpoint: {AZURE_OPENAI_ENDPOINT}\")\n",
    "print(f\"API Key: {AZURE_OPENAI_API_KEY[:8]}...{AZURE_OPENAI_API_KEY[-4:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d464ab1c",
   "metadata": {},
   "source": [
    "## Pipeline Configuration\n",
    "\n",
    "**What this does:** Configure the AI pipeline to create highlights tailored to your video content and preferences.\n",
    "\n",
    "**Key settings you control:**\n",
    "- ğŸ¥ **Video source** - Path to your video file\n",
    "- ğŸ¬ **Content type** - What kind of video you're analyzing (sports, keynote, etc.)\n",
    "- â±ï¸ **Target length** - How long you want your final highlight video\n",
    "- ğŸ¯ **Style preferences** - Clip density, transitions, effects\n",
    "- ğŸš€ **Personalization** - Anything specific you expect your highlights to pertain to\n",
    "\n",
    "**Important:** You must specify the `SOURCE_VIDEO_PATH` and choose the right `VIDEO_TYPE` before running the notebook. The VIDEO_TYPE determines what events the AI looks for (e.g., \"soccer\" finds goals and exciting plays, \"keynote\" finds key quotes and audience reactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¥ VIDEO CONFIGURATION\n",
    "\n",
    "import time\n",
    "# Replace with your actual video file path\n",
    "SOURCE_VIDEO_PATH = r\"\"  # e.g., r\"C:\\Users\\YourName\\Videos\\video.mp4\"\n",
    "\n",
    "# ğŸ“ OUTPUT DIRECTORY (will use the same directory as your video file by default)\n",
    "OUTPUT_DIR = os.path.dirname(os.path.abspath(SOURCE_VIDEO_PATH if SOURCE_VIDEO_PATH else __file__))\n",
    "\n",
    "# ğŸ¬ SCHEMA CONFIGURATION (schema types available in ./schemas/ folder)\n",
    "VIDEO_TYPE = \"soccer\"  # Options: \"soccer\", \"basketball\", \"football\", etc.\n",
    "\n",
    "# Path to the schema file (this will be auto-configured based on VIDEO_TYPE)\n",
    "SCHEMA_PATH = None  # This will be set automatically\n",
    "\n",
    "# ğŸ¯ HIGHLIGHT GENERATION PARAMETERS\n",
    "TARGET_DURATION_S = 60          # Target duration for the final highlight video in seconds\n",
    "CLIP_DENSITY = \"medium\"          # Options: \"low\", \"medium\", \"high\"\n",
    "PERSONALIZATION = \"none\"      # Any specific expectations from the highlight\n",
    "TRANSITION_TYPE = \"cut\"          # Options: \"cut\", \"fade\"\n",
    "SPEED_RAMP = False              # Whether to apply speed ramping\n",
    "ADD_CAPTIONS = False            # Whether to add captions (requires additional setup, keep FALSE)\n",
    "RESOLUTION = 720                # Output resolution height (720p, 1080p)\n",
    "HUMAN_IN_THE_LOOP_REVIEW = False # Whether to pause for human review of analyzer generation\n",
    "\n",
    "# ğŸš€ ADVANCED CONFIGURATION\n",
    "ANALYZER_ID = f\"highlight-analyzer-{int(time.time())}\"  # Unique analyzer ID\n",
    "\n",
    "# Validate configuration\n",
    "if not SOURCE_VIDEO_PATH or SOURCE_VIDEO_PATH == \"path/to/your/video.mp4\":\n",
    "    raise ValueError(\"âš ï¸ Please update SOURCE_VIDEO_PATH with your actual video file path!\")\n",
    "\n",
    "if not os.path.exists(SOURCE_VIDEO_PATH):\n",
    "    raise ValueError(f\"âŒ Video file not found: {SOURCE_VIDEO_PATH}\")\n",
    "\n",
    "print(f\"âœ… Configuration validated!\")\n",
    "print(f\"Source Video: {SOURCE_VIDEO_PATH}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Video Type: {VIDEO_TYPE}\")\n",
    "print(f\"Target Duration: {TARGET_DURATION_S}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bbb961",
   "metadata": {},
   "source": [
    "## 1. Generate and Activate Schema\n",
    "\n",
    "**What this does:** Creates a custom Azure AI analyzer that uses OpenAI reasoning to understand your specific video content and identify highlight-worthy moments based on your preferences.\n",
    "\n",
    "The schema generation process takes your `VIDEO_TYPE`, `CLIP_DENSITY`, `TARGET_DURATION_S`, and `PERSONALIZATION` settings and builds an intelligent analyzer that knows exactly what to look for in your video (e.g., goals in soccer, key quotes in presentations, exciting moments in gameplay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import schema_manager\n",
    "\n",
    "try:\n",
    "    SCHEMA_PATH = schema_manager.activate_schema(\n",
    "        VIDEO_TYPE, \n",
    "        CLIP_DENSITY,\n",
    "        TARGET_DURATION_S,\n",
    "        PERSONALIZATION,\n",
    "        human_in_the_loop_review=HUMAN_IN_THE_LOOP_REVIEW\n",
    "    )\n",
    "    print(f\"Schema activated: {SCHEMA_PATH}\")\n",
    "    \n",
    "    # Display schema content\n",
    "    with open(SCHEMA_PATH, 'r') as f:\n",
    "        import json\n",
    "        print(json.dumps(json.load(f), indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error activating schema: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8262d4",
   "metadata": {},
   "source": [
    "## 2. Analyze Video\n",
    "\n",
    "Now, we submit the video to Azure Content Understanding (CU) for analysis using the custom schema we just generated. This step can take a long time depending on the length of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332ce09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the analysis module\n",
    "from run_video_analysis import run_analysis\n",
    "\n",
    "# Get Azure AI endpoint from environment\n",
    "AZURE_AI_ENDPOINT = os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "if not AZURE_AI_ENDPOINT:\n",
    "    raise ValueError(\"âš ï¸ AZURE_AI_ENDPOINT environment variable not set! Please check your .env file.\")\n",
    "\n",
    "print(\"ğŸ¬ Starting video analysis...\")\n",
    "print(f\"Video: {os.path.basename(SOURCE_VIDEO_PATH)}\")\n",
    "print(f\"Schema: {os.path.basename(SCHEMA_PATH)}\")\n",
    "\n",
    "# Run the analysis with all parameters passed explicitly\n",
    "success, ANALYSIS_RESULT_PATH, error = run_analysis(\n",
    "    video_path=SOURCE_VIDEO_PATH,\n",
    "    schema_path=SCHEMA_PATH,\n",
    "    azure_ai_endpoint=AZURE_AI_ENDPOINT,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    analyzer_id=ANALYZER_ID\n",
    ")\n",
    "\n",
    "if not success:\n",
    "    raise Exception(f\"âŒ Video analysis failed: {error}\")\n",
    "\n",
    "print(f\"âœ… Video analysis completed successfully!\")\n",
    "print(f\"ğŸ“„ Results saved to: {ANALYSIS_RESULT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b5c9f5",
   "metadata": {},
   "source": [
    "## 3. Parse and Pre-filter Segments\n",
    "\n",
    "Now we receive the output from Azure CU, which describes all the video frames and their content. We apply rule-based filtering to remove frames that are not worthy enough to be part of the highlights, based on the schema's scoring criteria. This creates a simplified list of potential highlight clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_parser\n",
    "import json\n",
    "\n",
    "try:\n",
    "    PREFILTERED_SEGMENTS_PATH, _, _ = json_parser.process_json(\n",
    "        input_path=ANALYSIS_RESULT_PATH\n",
    "    )\n",
    "    print(f\"Segments parsed and pre-filtered. Results saved to: {PREFILTERED_SEGMENTS_PATH}\")\n",
    "\n",
    "    # Display the pre-filtered segments\n",
    "    with open(PREFILTERED_SEGMENTS_PATH, 'r') as f:\n",
    "        segments_data = json.load(f)\n",
    "        print(json.dumps(segments_data, indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error parsing segments: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5024c",
   "metadata": {},
   "source": [
    "## 4. Highlight Edits Filtering\n",
    "\n",
    "Now we send the parsed output to OpenAI o1 for advanced reasoning and intelligent timestamp selection. The AI analyzes all the potential highlight clips and selects the specific timestamps that will create the most compelling custom highlights catered to your preferences, meeting the `TARGET_DURATION_S` and arranging them in an optimal narrative order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167758a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import build_highlight\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Set the global variables that build_highlight.main() expects\n",
    "    build_highlight.SEGMENTS_PATH = PREFILTERED_SEGMENTS_PATH\n",
    "    build_highlight.RUNTIME_S = TARGET_DURATION_S\n",
    "    \n",
    "    # Call main function (no parameters needed)\n",
    "    build_highlight.main()\n",
    "    \n",
    "    # The output file is hardcoded to \"final_highlight_result.json\" in the same directory\n",
    "    HIGHLIGHT_PLAN_PATH = \"final_highlight_result.json\"\n",
    "    print(f\"Highlight plan created. Saved to: {HIGHLIGHT_PLAN_PATH}\")\n",
    "\n",
    "    # Display the highlight plan\n",
    "    with open(HIGHLIGHT_PLAN_PATH, 'r') as f:\n",
    "        plan_data = json.load(f)\n",
    "        print(json.dumps(plan_data, indent=2))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error building highlight plan: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec0d8bc",
   "metadata": {},
   "source": [
    "## 5. Stitch Video Clips\n",
    "\n",
    "From the feedback we received from OpenAI's reasoning, we now work on stitching together the custom user highlights. This final step takes the intelligently selected timestamps and creates a seamless highlight video tailored to your preferences using FFmpeg for video editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9116c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Stitching using FFmpeg\n",
    "import video_stitching_ffmpeg\n",
    "import os\n",
    "\n",
    "# Define output path for the highlight video\n",
    "OUTPUT_HIGHLIGHT_PATH = os.path.join(OUTPUT_DIR, \"highlight.mp4\")\n",
    "\n",
    "print(\"ğŸ¬ Running video stitching with FFmpeg...\")\n",
    "print(f\"ğŸ“„ Input video: {SOURCE_VIDEO_PATH}\")\n",
    "print(f\"ğŸ“„ Highlight plan: {HIGHLIGHT_PLAN_PATH}\")\n",
    "print(f\"ğŸ“„ Output video: {OUTPUT_HIGHLIGHT_PATH}\")\n",
    "\n",
    "try:\n",
    "    result = video_stitching_ffmpeg.stitch_video(\n",
    "        video_path=SOURCE_VIDEO_PATH,\n",
    "        plan_path=HIGHLIGHT_PLAN_PATH,\n",
    "        output_path=OUTPUT_HIGHLIGHT_PATH,\n",
    "        transition=TRANSITION_TYPE,\n",
    "        speed_ramp=SPEED_RAMP,\n",
    "        resolution=RESOLUTION\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(f\"âœ… SUCCESS: Highlight video created at {result}\")\n",
    "        \n",
    "        # Check file info\n",
    "        if os.path.exists(result):\n",
    "            size_mb = os.path.getsize(result) / (1024*1024)\n",
    "            print(f\"   ğŸ“ File size: {size_mb:.2f} MB\")\n",
    "            print(f\"   ğŸ“ Full path: {result}\")\n",
    "        print(f\"\\nğŸ‰ Highlight generation pipeline completed successfully!\")\n",
    "    else:\n",
    "        print(\"âŒ Video stitching failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR during video stitching: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
