{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2524c799",
   "metadata": {},
   "source": [
    "# AutoHighlight: AI-Powered Video Highlights Generation\n",
    "\n",
    "Welcome! **AutoHighlight** is an innovative AI-powered pipeline that automatically generates compelling video highlights from any video content. By combining Azure Content Understanding with OpenAI's advanced reasoning capabilities, it intelligently identifies the most engaging moments and creates professional highlight reels tailored to your specific needs.\n",
    "\n",
    "---\n",
    "\n",
    "## What AutoHighlight Does\n",
    "\n",
    "Transform hours of video content into captivating highlights in minutes:\n",
    "\n",
    "- **Smart Content Analysis:** Uses Azure Content Understanding to analyze every frame and identify key moments\n",
    "- **AI-Powered Selection:** Leverages OpenAI o3's reasoning to select the most compelling clips based on your preferences\n",
    "- **Automated Editing:** Creates seamless highlight videos with professional transitions and timing\n",
    "- **Personalized Results:** Tailors highlights to specific content types (sports, presentations, events) and your custom preferences\n",
    "- **Production Ready:** Robust pipeline suitable for content creators, marketers, educators, and media professionals\n",
    "\n",
    "Perfect for creating highlight reels from sports events, keynote presentations, product demos, educational content, entertainment shows, and more!\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Multi-Domain Support:** Pre-configured schemas for sports (soccer, basketball, football), presentations, events, and custom content types\n",
    "- **Intelligent Timestamp Selection:** AI reasoning identifies the most impactful moments based on content analysis\n",
    "- **Flexible Output Control:** Customize highlight duration, clip density, transitions, and personalization preferences\n",
    "- **Professional Video Processing:** FFmpeg-based editing with support for various resolutions and effects\n",
    "- **User-Friendly Interface:** Jupyter notebook with step-by-step guidance and clear documentation\n",
    "\n",
    "The pipeline uses the latest Azure AI Content Understanding API (`2025-05-01-preview`) combined with OpenAI's most advanced reasoning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a266f2",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "1. Follow [README](../README.md#configure-azure-ai-service-resource) to create essential resource that will be used in this sample.\n",
    "2. Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a3981",
   "metadata": {},
   "source": [
    "> Note: \n",
    "> - We've tested with GPT `o3` model. However, the API call could be blocked by content filter. You may need to modify the content filter of GPT model deployment. [How to configure content filters](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/content-filters?utm_source=chatgpt.com)\n",
    "> - All customers have the ability to modify the content filters and configure the severity thresholds (low, medium, high). Approval is required for turning the content filters partially or fully off. Managed customers only may apply for full content filtering control via this form: [Azure OpenAI Limited Access Review: Modified Content Filters](https://ncv.microsoft.com/uEfCgnITdR). At this time, it is not possible to become a managed customer.\n",
    ">   - This form is also used to register for approval to use Azure AI Content Understanding with Content Filtering turned off. The subscription IDs with approved Modified Content Filtering will impact the Azure AI Content Understanding output. To learn more about Azure AI Content Understanding, see the documentation [here](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df751e",
   "metadata": {},
   "source": [
    "## Azure AAD Login\n",
    "If you haven't done so, please authenticate by running **'az login'** through the terminal. This credentials are used to validate that you have access to the resources you defined above.\n",
    "\n",
    "Make sure you have Azure CLI installed on your system. To install:\n",
    "```bash\n",
    "curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d56d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Authehticate if you are running this notebook for the first time.\n",
    "import subprocess\n",
    "subprocess.run(\"az login\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302c4be",
   "metadata": {},
   "source": [
    "## Load Environment Variables and Create Clients\n",
    "\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class providing functions to interact with the Content Understanding API. Before the official release of the Content Understanding SDK, it acts as a lightweight SDK. Please fill in the constants **AZURE_AI_SERVICE_ENDPOINT** and **AZURE_AI_SERVICE_API_VERSION** with your Azure AI Service information. Optionally, you may provide **AZURE_AI_SERVICE_API_KEY** if your setup requires key-based authentication.\n",
    "\n",
    "> ⚠️ Important:\n",
    "Please update the code below to match your Azure authentication method.\n",
    "Look for the `# IMPORTANT` comments and modify those sections accordingly.\n",
    "Skipping this step may cause the sample to not run correctly.\n",
    "\n",
    "> ⚠️ Note: Using a subscription key works, but using a token provider with Azure Active Directory (AAD) is safer and highly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "# add the parent directory to the path to use shared modules\n",
    "sys.path.append(\n",
    "    str(parent_dir)\n",
    ")\n",
    "from python.utility import OpenAIAssistant\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2025-05-01-preview\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in the \".env\" file if not using token authentication.\n",
    "AZURE_AI_SERVICE_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in the \".env\" file if not using token authentication.\n",
    "AZURE_OPENAI_API_API_KEY = os.getenv(\"AZURE_AI_API_KEY\", None)\n",
    "\n",
    "# Create an OpenAI Assistant to interact with Azure OpenAI\n",
    "openai_assistant = OpenAIAssistant(\n",
    "    aoai_end_point=AZURE_OPENAI_ENDPOINT,\n",
    "    aoai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "    deployment_name=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "    aoai_api_key=AZURE_OPENAI_API_API_KEY,\n",
    ")\n",
    "\n",
    "# Create a token provider using DefaultAzureCredential for AAD authentication for AzureContentUnderstandingClient\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "# Create the Content Understanding (CU) client\n",
    "cu_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    # IMPORTANT: Comment out token_provider if using subscription key\n",
    "    token_provider=token_provider,\n",
    "    # IMPORTANT: Uncomment this if using subscription key\n",
    "    # subscription_key=AZURE_AI_SERVICE_API_KEY,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/video_chapters_dynamic\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc1839",
   "metadata": {},
   "source": [
    "## VIDEO CONFIGURATION\n",
    "\n",
    "Use the following variables to configure the auto-highlight generation tailored to your video content and preferences.\n",
    "\n",
    "Key settings you control:\n",
    "\n",
    "- **🎥 Video source**: Path to your video file\n",
    "- **🎬 Content type**: What kind of video you're analyzing (sports, keynote, etc.)\n",
    "- **⏱️ Target length**: How long you want your final highlight video\n",
    "- **🎯 Style preferences**: Clip density, transitions, effects\n",
    "- **🚀 Personalization**: Anything specific you expect your highlights to pertain to\n",
    "\n",
    "> Important: You must specify the `SOURCE_VIDEO_PATH` and choose the right `VIDEO_TYPE` before running the notebook. The `VIDEO_TYPE` determines what events the AI looks for (e.g., \"soccer\" finds goals and exciting plays, \"keynote\" finds key quotes and audience reactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎥 VIDEO CONFIGURATION\n",
    "\n",
    "import time\n",
    "import uuid\n",
    "# Replace with your actual video file path\n",
    "SOURCE_VIDEO_PATH = \"/workspaces/azure-ai-content-understanding-with-azure-openai-python/data/FlightSimulator.mp4\"  # Update this path to your video file\n",
    "\n",
    "# 📁 OUTPUT DIRECTORY (will create output folder as default)\n",
    "OUTPUT_DIR = parent_dir / \"output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 🎬 SCHEMA CONFIGURATION (schema types available in ./schemas/ folder)\n",
    "VIDEO_TYPE = \"demo\"  # Options: \"soccer\", \"basketball\", \"football\", etc.\n",
    "\n",
    "# Path to the schema file (this will be auto-configured based on VIDEO_TYPE)\n",
    "ANALYZER_DIR = \"/workspaces/azure-ai-content-understanding-with-azure-openai-python/analyzer_templates/auto_highlight_analyzers\"\n",
    "\n",
    "# 🎯 HIGHLIGHT GENERATION PARAMETERS\n",
    "TARGET_DURATION_S = 20           # Target duration for the final highlight video in seconds\n",
    "CLIP_DENSITY = 1.0               # Density of clips to include (1.0 = normal, >1.0 = more clips, <1.0 = fewer clips)\n",
    "PERSONALIZATION = \"none\"         # Any specific expectations from the highlight\n",
    "TRANSITION_TYPE = \"cut\"          # Options: \"cut\", \"fade\"\n",
    "SPEED_RAMP = False               # Whether to apply speed ramping\n",
    "ADD_CAPTIONS = False             # Whether to add captions (requires additional setup, keep FALSE)\n",
    "RESOLUTION = 720                 # Output resolution height (720p, 1080p)\n",
    "HUMAN_IN_THE_LOOP_REVIEW = True # Whether to pause for human review of analyzer generation\n",
    "\n",
    "# Analyzer ID configuration\n",
    "ANALYZER_ID = f\"highlight-analyzer-{str(uuid.uuid4())[:8]}-{int(time.time())}\"  # Unique analyzer ID with UUID\n",
    "\n",
    "\n",
    "if not os.path.exists(SOURCE_VIDEO_PATH):\n",
    "    raise ValueError(f\"❌ Video file not found: {SOURCE_VIDEO_PATH}\")\n",
    "\n",
    "print(\"✅ Configuration loaded successfully!\")\n",
    "print(\"🔧 Configuration Details:\")\n",
    "print(f\"Source Video: {SOURCE_VIDEO_PATH}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Analyzer Directory: {ANALYZER_DIR}\")\n",
    "print(f\"Video Type: {VIDEO_TYPE}\")\n",
    "print(f\"Target Duration: {TARGET_DURATION_S}s\")\n",
    "print(f\"Analyzer ID: {ANALYZER_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4b8b7",
   "metadata": {},
   "source": [
    "## 1. Generate and Activate Schema\n",
    "*What this does:* Creates a custom Azure AI analyzer that uses OpenAI reasoning to understand your specific video content and identify highlight-worthy moments based on your preferences.\n",
    "\n",
    "The schema generation process takes your VIDEO_TYPE, CLIP_DENSITY, TARGET_DURATION_S, and PERSONALIZATION settings and builds an intelligent analyzer that knows exactly what to look for in your video (e.g., goals in soccer, key quotes in presentations, exciting moments in gameplay)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac7221",
   "metadata": {},
   "source": [
    "### Schema Review GUI Instruction\n",
    "If you use schema generation and set HUMAN_IN_THE_LOOP_REVIEW as `True`, the Schema Review GUI will be initiated. Here is the instroduction of the Schema Review GUI.\n",
    "\n",
    "1. **Review Schema Fields:**\n",
    "   - Each row shows the field name, its type, and its description.\n",
    "   - A checkbox next to each field lets you include or exclude it from the schema.\n",
    "     - Uncheck the box next to any field you do not want in the final schema.\n",
    "     - Checked properties will be kept; unchecked ones will be removed.\n",
    "2. **Save Button:**\n",
    "   - Click \"Save\" to:\n",
    "     - Save your current selection to the schema file.\n",
    "     - Refresh the list of fields to reflect your changes.\n",
    "     - See a confirmation popup that the schema was saved.\n",
    "3. **Add Manual Field:**\n",
    "   - Click \"Add Manual Field\" to open a dialog for adding a new field.\n",
    "     - Enter the field name.\n",
    "     - Choose the type (string, date, time, number, integer).\n",
    "     - Choose the method (classify or generate).\n",
    "     - Enter a description.\n",
    "     - If \"classify\" is selected, you can enter enum options (comma-separated).\n",
    "   - Click \"OK\" to add the field to the schema and update the list.\n",
    "4. **Add GPT Field:**\n",
    "   - Click \"Add GPT Field\" to let GPT suggest a field.\n",
    "     - Enter a description of the field you want.\n",
    "     - The assistant will generate and add a field based on your description.\n",
    "     - (Requires an OpenAI assistant to be provided.)\n",
    "5. **Done Button:**\n",
    "   - Click \"Done\" when you are finished reviewing.\n",
    "     - The schema will be saved with your final selection.\n",
    "     - The GUI will close.\n",
    "     - A confirmation popup will appear.\n",
    "\n",
    "**Tips:**\n",
    "- You can add, remove, or edit fields as many times as needed before clicking \"Done\".\n",
    "- Use \"Save\" to checkpoint your progress without closing the window.\n",
    "- If you make a mistake, you can re-add fields or adjust your selections before finishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.auto_highlight_utility import activate_schema\n",
    "\n",
    "schema_path = activate_schema(\n",
    "    video_type=VIDEO_TYPE,\n",
    "    analyzer_dir=ANALYZER_DIR,\n",
    "    openai_assistant=openai_assistant,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    clip_density=CLIP_DENSITY,\n",
    "    target_duration_s=TARGET_DURATION_S, \n",
    "    personalization=PERSONALIZATION,\n",
    "    human_in_the_loop_review=HUMAN_IN_THE_LOOP_REVIEW)\n",
    "\n",
    "print(f\"Schema activated: {schema_path}\")\n",
    "\n",
    "# Display schema content\n",
    "with open(schema_path, 'r') as f:\n",
    "    import json\n",
    "    print(json.dumps(json.load(f), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191deeb8",
   "metadata": {},
   "source": [
    "## 2. Analyze Video\n",
    "Now, we submit the video to Azure Content Understanding (CU) for analysis using the custom schema we just generated. This step can take a long time depending on the length of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07836d",
   "metadata": {},
   "source": [
    "### Create a Custom Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9dc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Use the client to create an analyzer\n",
    "response = cu_client.begin_create_analyzer(ANALYZER_ID, analyzer_template_path=schema_path)\n",
    "result = cu_client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ec78e",
   "metadata": {},
   "source": [
    "### Use the created analyzer to extract video content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147eb3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the video for content analysis\n",
    "response = cu_client.begin_analyze(ANALYZER_ID, file_location=SOURCE_VIDEO_PATH)\n",
    "\n",
    "# Wait for the analysis to complete and get the content analysis result\n",
    "video_cu_result = cu_client.poll_result(\n",
    "    response, timeout_seconds=3600)  # 1 hour timeout for long videos\n",
    "\n",
    "# Print the content analysis result\n",
    "print(f\"Video Content Understanding result: \", video_cu_result)\n",
    "\n",
    "cu_result_filename = f\"{os.path.basename(SOURCE_VIDEO_PATH)}.json\"\n",
    "cu_result_path = os.path.join(OUTPUT_DIR, cu_result_filename)\n",
    "\n",
    "# Save results\n",
    "with open(cu_result_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(video_cu_result, f, indent=2)\n",
    "\n",
    "print(f\"Video analysis complete. Results saved to: {cu_result_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e05b6",
   "metadata": {},
   "source": [
    "### Optional - Delete the analyzer if it is no longer needed\n",
    "Note: In production environments, analyzers are typically kept for reuse rather than deleting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f765351",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a390ab1",
   "metadata": {},
   "source": [
    "## 3. Parse and Pre-filter Segments\n",
    "\n",
    "Now we receive the output from Azure CU, which describes all the video frames and their content. We apply rule-based filtering to remove frames that are not worthy enough to be part of the highlights, based on the schema's scoring criteria. This creates a simplified list of potential highlight clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854073d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.auto_highlight_utility import get_filtered_segments\n",
    "\n",
    "prefiltered_segments_path = get_filtered_segments(input_path=cu_result_path)\n",
    "print(f\"Segments parsed and pre-filtered. Results saved to: {prefiltered_segments_path}\")\n",
    "\n",
    "# Display the pre-filtered segments\n",
    "with open(prefiltered_segments_path, 'r') as f:\n",
    "    segments_data = json.load(f)\n",
    "    print(json.dumps(segments_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b629e9",
   "metadata": {},
   "source": [
    "## 4. Highlight Edits Filtering\n",
    "Now we send the parsed output to OpenAI o1 for advanced reasoning and intelligent timestamp selection. The AI analyzes all the potential highlight clips and selects the specific timestamps that will create the most compelling custom highlights catered to your preferences, meeting the `TARGET_DURATION_S` and arranging them in an optimal narrative order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.utility import get_highlight_plan\n",
    "\n",
    "result = get_highlight_plan(\n",
    "    openai_assistant=openai_assistant,\n",
    "    segments=segments_data,\n",
    "    video_type=VIDEO_TYPE,\n",
    "    target_duration_s=TARGET_DURATION_S,\n",
    "    clip_density=CLIP_DENSITY,\n",
    "    personalization=PERSONALIZATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = result.model_dump()\n",
    "\n",
    "# Save to JSON file\n",
    "highlight_plan_file = \"highlight_plan.json\"\n",
    "highlight_plan_path = os.path.join(OUTPUT_DIR, highlight_plan_file)\n",
    "with open(highlight_plan_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved JSON to {highlight_plan_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e50fe",
   "metadata": {},
   "source": [
    "## 5. Stitch Video Clips\n",
    "From the feedback we received from OpenAI's reasoning, we now work on stitching together the custom user highlights. This final step takes the intelligently selected timestamps and creates a seamless highlight video tailored to your preferences using FFmpeg for video editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Stitching using FFmpeg\n",
    "from python.video_modification_utility import stitch_video\n",
    "\n",
    "# Define output path for the highlight video\n",
    "OUTPUT_HIGHLIGHT_PATH = os.path.join(OUTPUT_DIR, \"highlight.mp4\")\n",
    "\n",
    "print(\"🎬 Running video stitching with FFmpeg...\")\n",
    "print(f\"📄 Input video: {SOURCE_VIDEO_PATH}\")\n",
    "print(f\"📄 Highlight plan: {highlight_plan_path}\")\n",
    "print(f\"📄 Output video: {OUTPUT_HIGHLIGHT_PATH}\")\n",
    "\n",
    "try:\n",
    "    result = stitch_video(\n",
    "        video_path=SOURCE_VIDEO_PATH,\n",
    "        plan_path=highlight_plan_path,\n",
    "        output_path=OUTPUT_HIGHLIGHT_PATH,\n",
    "        transition=TRANSITION_TYPE,\n",
    "        speed_ramp=SPEED_RAMP,\n",
    "        resolution=RESOLUTION\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(f\"✅ SUCCESS: Highlight video created at {result}\")\n",
    "        \n",
    "        # Check file info\n",
    "        if os.path.exists(result):\n",
    "            size_mb = os.path.getsize(result) / (1024*1024)\n",
    "            print(f\"   📁 File size: {size_mb:.2f} MB\")\n",
    "            print(f\"   📁 Full path: {result}\")\n",
    "        print(f\"\\n🎉 Highlight generation pipeline completed successfully!\")\n",
    "    else:\n",
    "        print(\"❌ Video stitching failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR during video stitching: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
