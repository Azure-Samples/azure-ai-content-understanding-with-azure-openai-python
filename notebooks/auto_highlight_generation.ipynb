{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2524c799",
   "metadata": {},
   "source": [
    "# AutoHighlight\n",
    "This notebook generates video highlights from a video file using Azure AI services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a266f2",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "1. Follow [README](../README.md#configure-azure-ai-service-resource) to create essential resource that will be used in this sample.\n",
    "2. Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302c4be",
   "metadata": {},
   "source": [
    "## Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "# add the parent directory to the path to use shared modules\n",
    "sys.path.append(\n",
    "    str(parent_dir)\n",
    ")\n",
    "\n",
    "from python.utility import OpenAIAssistant\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2025-05-01-preview\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Create an OpenAI Assistant to interact with Azure OpenAI\n",
    "openai_assistant = OpenAIAssistant(\n",
    "    aoai_end_point=AZURE_OPENAI_ENDPOINT,\n",
    "    aoai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "    deployment_name=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "    aoai_api_key=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02a0c4",
   "metadata": {},
   "source": [
    "If you haven't done so, please authenticate by running **'az login'** through the terminal. This credentials are used to validate that you have access to the resources you defined above.\n",
    "\n",
    "Make sure you have Azure CLI installed on your system. To install:\n",
    "```bash\n",
    "curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5800017",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Authehticate if you are running this notebook for the first time.\n",
    "import subprocess\n",
    "subprocess.run(\"az login\", shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc1839",
   "metadata": {},
   "source": [
    "## VIDEO CONFIGURATION\n",
    "\n",
    "Use the following variables to configure the auto-highlight generation tailored to your video content and preferences.\n",
    "\n",
    "Key settings you control:\n",
    "\n",
    "- **ðŸŽ¥ Video source**: Path to your video file\n",
    "- **ðŸŽ¬ Content type**: What kind of video you're analyzing (sports, keynote, etc.)\n",
    "- **â±ï¸ Target length**: How long you want your final highlight video\n",
    "- **ðŸŽ¯ Style preferences**: Clip density, transitions, effects\n",
    "- **ðŸš€ Personalization**: Anything specific you expect your highlights to pertain to\n",
    "\n",
    "> Important: You must specify the `SOURCE_VIDEO_PATH` and choose the right `VIDEO_TYPE` before running the notebook. The `VIDEO_TYPE` determines what events the AI looks for (e.g., \"soccer\" finds goals and exciting plays, \"keynote\" finds key quotes and audience reactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¥ VIDEO CONFIGURATION\n",
    "\n",
    "import time\n",
    "import uuid\n",
    "# Replace with your actual video file path\n",
    "SOURCE_VIDEO_PATH = \"/workspaces/azure-ai-content-understanding-with-azure-openai-python/data/FlightSimulator.mp4\"  # Update this path to your video file\n",
    "\n",
    "# ðŸ“ OUTPUT DIRECTORY (will create output folder as default)\n",
    "OUTPUT_DIR = parent_dir / \"output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ðŸŽ¬ SCHEMA CONFIGURATION (schema types available in ./schemas/ folder)\n",
    "VIDEO_TYPE = \"demo\"  # Options: \"soccer\", \"basketball\", \"football\", etc.\n",
    "\n",
    "# Path to the schema file (this will be auto-configured based on VIDEO_TYPE)\n",
    "ANALYZER_DIR = \"/workspaces/azure-ai-content-understanding-with-azure-openai-python/analyzer_templates/auto_highlight_analyzers\"\n",
    "\n",
    "# ðŸŽ¯ HIGHLIGHT GENERATION PARAMETERS\n",
    "TARGET_DURATION_S = 20           # Target duration for the final highlight video in seconds\n",
    "CLIP_DENSITY = 1.0               # Density of clips to include (1.0 = normal, >1.0 = more clips, <1.0 = fewer clips)\n",
    "PERSONALIZATION = \"none\"         # Any specific expectations from the highlight\n",
    "TRANSITION_TYPE = \"cut\"          # Options: \"cut\", \"fade\"\n",
    "SPEED_RAMP = False               # Whether to apply speed ramping\n",
    "ADD_CAPTIONS = False             # Whether to add captions (requires additional setup, keep FALSE)\n",
    "RESOLUTION = 720                 # Output resolution height (720p, 1080p)\n",
    "HUMAN_IN_THE_LOOP_REVIEW = True # Whether to pause for human review of analyzer generation\n",
    "\n",
    "# Analyzer ID configuration\n",
    "ANALYZER_ID = f\"highlight-analyzer-{str(uuid.uuid4())[:8]}-{int(time.time())}\"  # Unique analyzer ID with UUID\n",
    "\n",
    "\n",
    "if not os.path.exists(SOURCE_VIDEO_PATH):\n",
    "    raise ValueError(f\"âŒ Video file not found: {SOURCE_VIDEO_PATH}\")\n",
    "\n",
    "print(\"âœ… Configuration loaded successfully!\")\n",
    "print(\"ðŸ”§ Configuration Details:\")\n",
    "print(f\"Source Video: {SOURCE_VIDEO_PATH}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Analyzer Directory: {ANALYZER_DIR}\")\n",
    "print(f\"Video Type: {VIDEO_TYPE}\")\n",
    "print(f\"Target Duration: {TARGET_DURATION_S}s\")\n",
    "print(f\"Analyzer ID: {ANALYZER_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4b8b7",
   "metadata": {},
   "source": [
    "## 1. Generate and Activate Schema\n",
    "*What this does:* Creates a custom Azure AI analyzer that uses OpenAI reasoning to understand your specific video content and identify highlight-worthy moments based on your preferences.\n",
    "\n",
    "The schema generation process takes your VIDEO_TYPE, CLIP_DENSITY, TARGET_DURATION_S, and PERSONALIZATION settings and builds an intelligent analyzer that knows exactly what to look for in your video (e.g., goals in soccer, key quotes in presentations, exciting moments in gameplay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.auto_highlight_utility import activate_schema\n",
    "\n",
    "schema_path = activate_schema(\n",
    "    video_type=VIDEO_TYPE,\n",
    "    analyzer_dir=ANALYZER_DIR,\n",
    "    openai_assistant=openai_assistant,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    clip_density=CLIP_DENSITY,\n",
    "    target_duration_s=TARGET_DURATION_S, \n",
    "    personalization=PERSONALIZATION,\n",
    "    human_in_the_loop_review=HUMAN_IN_THE_LOOP_REVIEW)\n",
    "\n",
    "print(f\"Schema activated: {schema_path}\")\n",
    "\n",
    "# Display schema content\n",
    "with open(schema_path, 'r') as f:\n",
    "    import json\n",
    "    print(json.dumps(json.load(f), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191deeb8",
   "metadata": {},
   "source": [
    "## 2. Analyze Video\n",
    "Now, we submit the video to Azure Content Understanding (CU) for analysis using the custom schema we just generated. This step can take a long time depending on the length of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07836d",
   "metadata": {},
   "source": [
    "### Create a Custom Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9dc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "# Create the Content Understanding (CU) client\n",
    "cu_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/video_chapters_dynamic\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")\n",
    "\n",
    "# Use the client to create an analyzer\n",
    "response = cu_client.begin_create_analyzer(ANALYZER_ID, analyzer_template_path=schema_path)\n",
    "result = cu_client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ec78e",
   "metadata": {},
   "source": [
    "### Use the created analyzer to extract video content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147eb3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the video for content analysis\n",
    "response = cu_client.begin_analyze(ANALYZER_ID, file_location=SOURCE_VIDEO_PATH)\n",
    "\n",
    "# Wait for the analysis to complete and get the content analysis result\n",
    "video_cu_result = cu_client.poll_result(\n",
    "    response, timeout_seconds=3600)  # 1 hour timeout for long videos\n",
    "\n",
    "# Print the content analysis result\n",
    "print(f\"Video Content Understanding result: \", video_cu_result)\n",
    "\n",
    "cu_result_filename = f\"{os.path.basename(SOURCE_VIDEO_PATH)}.json\"\n",
    "cu_result_path = os.path.join(OUTPUT_DIR, cu_result_filename)\n",
    "\n",
    "# Save results\n",
    "with open(cu_result_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(video_cu_result, f, indent=2)\n",
    "\n",
    "print(f\"Video analysis complete. Results saved to: {cu_result_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17b05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip analysis TODO: remove\n",
    "cu_result_filename = f\"{os.path.basename(SOURCE_VIDEO_PATH)}.json\"\n",
    "cu_result_path = os.path.join(OUTPUT_DIR, cu_result_filename)\n",
    "cu_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e05b6",
   "metadata": {},
   "source": [
    "### Optional - Delete the analyzer if it is no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f765351",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a390ab1",
   "metadata": {},
   "source": [
    "## 3. Parse and Pre-filter Segments\n",
    "\n",
    "Now we receive the output from Azure CU, which describes all the video frames and their content. We apply rule-based filtering to remove frames that are not worthy enough to be part of the highlights, based on the schema's scoring criteria. This creates a simplified list of potential highlight clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854073d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.auto_highlight_utility import get_filtered_segments\n",
    "\n",
    "prefiltered_segments_path = get_filtered_segments(input_path=cu_result_path)\n",
    "print(f\"Segments parsed and pre-filtered. Results saved to: {prefiltered_segments_path}\")\n",
    "\n",
    "# Display the pre-filtered segments\n",
    "with open(prefiltered_segments_path, 'r') as f:\n",
    "    segments_data = json.load(f)\n",
    "    print(json.dumps(segments_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b629e9",
   "metadata": {},
   "source": [
    "## 4. Highlight Edits Filtering\n",
    "Now we send the parsed output to OpenAI o1 for advanced reasoning and intelligent timestamp selection. The AI analyzes all the potential highlight clips and selects the specific timestamps that will create the most compelling custom highlights catered to your preferences, meeting the `TARGET_DURATION_S` and arranging them in an optimal narrative order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.utility import get_highlight_plan\n",
    "\n",
    "result = get_highlight_plan(\n",
    "    openai_assistant=openai_assistant,\n",
    "    segments=segments_data,\n",
    "    video_type=VIDEO_TYPE,\n",
    "    target_duration_s=TARGET_DURATION_S,\n",
    "    clip_density=CLIP_DENSITY,\n",
    "    personalization=PERSONALIZATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = result.model_dump()\n",
    "\n",
    "# Save to JSON file\n",
    "highlight_plan_file = \"highlight_plan.json\"\n",
    "highlight_plan_path = os.path.join(OUTPUT_DIR, highlight_plan_file)\n",
    "with open(highlight_plan_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved JSON to {highlight_plan_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e50fe",
   "metadata": {},
   "source": [
    "## 5. Stitch Video Clips\n",
    "From the feedback we received from OpenAI's reasoning, we now work on stitching together the custom user highlights. This final step takes the intelligently selected timestamps and creates a seamless highlight video tailored to your preferences using FFmpeg for video editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Stitching using FFmpeg\n",
    "from python.video_modification_utility import stitch_video\n",
    "\n",
    "# Define output path for the highlight video\n",
    "OUTPUT_HIGHLIGHT_PATH = os.path.join(OUTPUT_DIR, \"highlight.mp4\")\n",
    "\n",
    "print(\"ðŸŽ¬ Running video stitching with FFmpeg...\")\n",
    "print(f\"ðŸ“„ Input video: {SOURCE_VIDEO_PATH}\")\n",
    "print(f\"ðŸ“„ Highlight plan: {highlight_plan_path}\")\n",
    "print(f\"ðŸ“„ Output video: {OUTPUT_HIGHLIGHT_PATH}\")\n",
    "\n",
    "try:\n",
    "    result = stitch_video(\n",
    "        video_path=SOURCE_VIDEO_PATH,\n",
    "        plan_path=highlight_plan_path,\n",
    "        output_path=OUTPUT_HIGHLIGHT_PATH,\n",
    "        transition=TRANSITION_TYPE,\n",
    "        speed_ramp=SPEED_RAMP,\n",
    "        resolution=RESOLUTION\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(f\"âœ… SUCCESS: Highlight video created at {result}\")\n",
    "        \n",
    "        # Check file info\n",
    "        if os.path.exists(result):\n",
    "            size_mb = os.path.getsize(result) / (1024*1024)\n",
    "            print(f\"   ðŸ“ File size: {size_mb:.2f} MB\")\n",
    "            print(f\"   ðŸ“ Full path: {result}\")\n",
    "        print(f\"\\nðŸŽ‰ Highlight generation pipeline completed successfully!\")\n",
    "    else:\n",
    "        print(\"âŒ Video stitching failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR during video stitching: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
