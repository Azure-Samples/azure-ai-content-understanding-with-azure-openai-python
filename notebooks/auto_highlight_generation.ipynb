{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2524c799",
   "metadata": {},
   "source": [
    "# AutoHighlight: AI-Powered Video Highlights Generation\n",
    "\n",
    "Welcome! **AutoHighlight** is an innovative AI-powered pipeline that automatically generates compelling video highlights from any video content. By combining Azure Content Understanding with OpenAI's advanced reasoning capabilities, it intelligently identifies the most engaging moments and creates professional highlight reels tailored to your specific needs.\n",
    "\n",
    "---\n",
    "\n",
    "## What AutoHighlight Does\n",
    "\n",
    "Transform hours of video content into captivating highlights within minutes:\n",
    "\n",
    "- **Smart Content Analysis:** Utilizes Azure Content Understanding to analyze every frame and identify key moments.\n",
    "- **AI-Powered Selection:** Leverages OpenAI o3's reasoning to select the most compelling clips based on your preferences.\n",
    "- **Automated Editing:** Creates seamless highlight videos with professional transitions and timing.\n",
    "- **Personalized Results:** Tailors highlights to specific content types (sports, presentations, events) and your custom preferences.\n",
    "- **Production Ready:** Robust pipeline suitable for content creators, marketers, educators, and media professionals.\n",
    "\n",
    "Perfect for creating highlight reels from sports events, keynote presentations, product demos, educational content, entertainment shows, and more!\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Multi-Domain Support:** Pre-configured schemas for sports (soccer, basketball, football), presentations, events, and custom content types.\n",
    "- **Intelligent Timestamp Selection:** AI reasoning identifies the most impactful moments based on content analysis.\n",
    "- **Flexible Output Control:** Customize highlight duration, clip density, transitions, and personalization preferences.\n",
    "- **Professional Video Processing:** Video editing supporting various resolutions and effects.\n",
    "- **User-Friendly Interface:** Jupyter notebook with step-by-step guidance and clear documentation.\n",
    "\n",
    "The pipeline uses the latest Azure AI Content Understanding API (`2025-05-01-preview`) combined with OpenAI's most advanced reasoning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a266f2",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "1. Please follow the [README](../README.md#configure-azure-ai-service-resource) to create the essential resources required for this sample.\n",
    "2. Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a3981",
   "metadata": {},
   "source": [
    "> Note: \n",
    "> - We've tested with the GPT `o3` model. However, API calls might be blocked by the content filter. Please consider modifying the content filter settings of your GPT model deployment if needed. [How to configure content filters](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/content-filters?utm_source=chatgpt.com)\n",
    "> - All customers have the ability to modify the content filters and configure the severity thresholds (low, medium, high). Approval is required for turning the content filters partially or fully off. Managed customers only may apply for full content filtering control via this form: [Azure OpenAI Limited Access Review: Modified Content Filters](https://ncv.microsoft.com/uEfCgnITdR). At this time, it is not possible to become a managed customer.\n",
    ">   - This form is also used to register for approval to use Azure AI Content Understanding with content filtering turned off. Subscription IDs approved for Modified Content Filtering will impact Azure AI Content Understanding output. To learn more about Azure AI Content Understanding, please see the documentation [here](https://learn.microsoft.com/en-us/azure/ai-services/content-understanding/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df751e",
   "metadata": {},
   "source": [
    "## Azure AAD Login\n",
    "### Option 1: Login with Azure Developer CLI (azd)\n",
    "If you haven't already, please authenticate by running **`azd auth login`** through the terminal. This credential is used to validate your access to the resources you defined in .env file and this notebook.\n",
    "\n",
    "Log in to Azure:\n",
    "\n",
    "```bash\n",
    "azd auth login\n",
    "```\n",
    "\n",
    "If this command doesnâ€™t work, try the device code login:\n",
    "\n",
    "```bash\n",
    "azd auth login --use-device-code\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c51f8",
   "metadata": {},
   "source": [
    "### Option 2: Login with Azure CLI (az)\n",
    "If you encounter authentication issue when using Azure Developer CLI (azd), please try login with Azure CLI (az).\n",
    "\n",
    "Please ensure you have the Azure CLI installed on your system. To install, run this in the terminal:\n",
    "```bash\n",
    "curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n",
    "```\n",
    "\n",
    "Log in to Azure:\n",
    "```bash\n",
    "az login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d2a60",
   "metadata": {},
   "source": [
    "### Option 3: Use Endpoint with API key\n",
    "If you would like to use endpoint with API key, please add **AZURE_AI_SERVICE_API_KEY** and **AZURE_OPENAI_API_API_KEY** into [.env](.env).\n",
    "\n",
    "> âš ï¸ Note: Using a subscription key works, but using a token provider with Azure Active Directory (AAD) is safer and strongly recommended for production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a302c4be",
   "metadata": {},
   "source": [
    "## Load Environment Variables and Create Clients\n",
    "\n",
    "> The [AzureContentUnderstandingClient](../python/content_understanding_client.py) is a utility class providing functions to interact with the Content Understanding API. Until the official release of the Content Understanding SDK, it functions as a lightweight SDK. Please fill in the constants **AZURE_AI_SERVICE_ENDPOINT** and **AZURE_AI_SERVICE_API_VERSION** with your Azure AI Service information. Optionally, provide **AZURE_AI_SERVICE_API_KEY** if your setup requires key-based authentication.\n",
    "\n",
    "> âš ï¸ Important:\n",
    "Please update the code below to match your Azure authentication method. Look for the `# IMPORTANT` comments and modify those sections accordingly. Skipping this step may cause the sample to not run correctly.\n",
    "\n",
    "> âš ï¸ Note: Using a subscription key works, but using a token provider with Azure Active Directory (AAD) is safer and highly recommended for production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "# Add the parent directory to the path to use shared modules\n",
    "sys.path.append(str(parent_dir))\n",
    "from python.utility import OpenAIAssistant\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2025-05-01-preview\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in the \".env\" file if not using token authentication.\n",
    "AZURE_AI_SERVICE_API_KEY = os.getenv(\"AZURE_AI_API_KEY\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-12-01-preview\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "# IMPORTANT: Replace with your actual subscription key or set it in the \".env\" file if not using token authentication.\n",
    "AZURE_OPENAI_API_API_KEY = os.getenv(\"AZURE_AI_API_KEY\", None)\n",
    "\n",
    "# Create an OpenAI Assistant to interact with Azure OpenAI\n",
    "openai_assistant = OpenAIAssistant(\n",
    "    aoai_end_point=AZURE_OPENAI_ENDPOINT,\n",
    "    aoai_api_version=AZURE_OPENAI_API_VERSION,\n",
    "    deployment_name=AZURE_OPENAI_CHAT_DEPLOYMENT_NAME,\n",
    "    aoai_api_key=AZURE_OPENAI_API_API_KEY,\n",
    ")\n",
    "\n",
    "# Create a token provider using DefaultAzureCredential for AAD authentication for AzureContentUnderstandingClient\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "# Create the Content Understanding (CU) client\n",
    "cu_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    # IMPORTANT: Comment out token_provider if using subscription key\n",
    "    token_provider=token_provider,\n",
    "    # IMPORTANT: Uncomment this if using subscription key\n",
    "    # subscription_key=AZURE_AI_SERVICE_API_KEY,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/video_chapters_dynamic\",  # This header is used for sample usage telemetry; please comment out this line if you wish to opt out.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc1839",
   "metadata": {},
   "source": [
    "## VIDEO CONFIGURATION\n",
    "\n",
    "Use the following variables to configure the auto-highlight generation tailored to your video content and preferences.\n",
    "\n",
    "Key settings you control:\n",
    "\n",
    "- **ðŸŽ¥ Video source:** Path to your video file\n",
    "- **ðŸŽ¬ Content type:** Type of video you're analyzing (sports, keynote, etc.)\n",
    "- **â±ï¸ Target length:** Desired length of the final highlight video\n",
    "- **ðŸŽ¯ Style preferences:** Clip density, transitions, effects\n",
    "- **ðŸš€ Personalization:** Any specific expectations you want your highlights to reflect\n",
    "\n",
    "> Important: Please specify the `SOURCE_VIDEO_PATH` and select the correct `VIDEO_TYPE` before running the notebook. The `VIDEO_TYPE` determines what events the AI looks for (e.g., \"soccer\" detects goals and exciting plays; \"keynote\" identifies key quotes and audience reactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d2243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¥ VIDEO CONFIGURATION\n",
    "\n",
    "import time\n",
    "import uuid\n",
    "# Replace with your actual video file path\n",
    "SOURCE_VIDEO_PATH = \"../data/FlightSimulator.mp4\"  # Please update this path to your video file\n",
    "\n",
    "# ðŸ“ OUTPUT DIRECTORY (will create output folder if it doesn't exist)\n",
    "OUTPUT_DIR = parent_dir / \"output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ðŸŽ¬ SCHEMA CONFIGURATION (schema types available in ./schemas/ folder)\n",
    "VIDEO_TYPE = \"demo\"  # Options: \"soccer\", \"basketball\", \"football\", etc.\n",
    "\n",
    "# Path to the analyzer template directory (auto-configured based on VIDEO_TYPE)\n",
    "ANALYZER_DIR = \"../analyzer_templates/auto_highlight_analyzers\"\n",
    "\n",
    "# ðŸŽ¯ HIGHLIGHT GENERATION PARAMETERS\n",
    "TARGET_DURATION_S = 20             # Target duration for the final highlight video in seconds\n",
    "CLIP_DENSITY = 1.0                 # Clip density to include (1.0 = normal, >1.0 = more clips, <1.0 = fewer clips)\n",
    "PERSONALIZATION = \"none\"           # Any specific expectations for the highlight\n",
    "TRANSITION_TYPE = \"cut\"            # Options: \"cut\", \"fade\"\n",
    "SPEED_RAMP = False                 # Apply speed ramping if True\n",
    "ADD_CAPTIONS = False               # Add captions (requires additional setup, default is False)\n",
    "RESOLUTION = 720                   # Output resolution height (e.g., 720p, 1080p)\n",
    "HUMAN_IN_THE_LOOP_REVIEW = False   # Pause for human review of analyzer generation if True, This is using Tkinter GUI which may not work on all environments \n",
    "\n",
    "# Analyzer ID configuration\n",
    "ANALYZER_ID = f\"highlight-analyzer-{str(uuid.uuid4())[:8]}-{int(time.time())}\"  # Unique analyzer ID using UUID\n",
    "\n",
    "\n",
    "if not os.path.exists(SOURCE_VIDEO_PATH):\n",
    "    raise ValueError(f\"âŒ Video file not found: {SOURCE_VIDEO_PATH}\")\n",
    "\n",
    "print(\"âœ… Configuration loaded successfully!\")\n",
    "print(\"ðŸ”§ Configuration Details:\")\n",
    "print(f\"Source Video: {SOURCE_VIDEO_PATH}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"Analyzer Directory: {ANALYZER_DIR}\")\n",
    "print(f\"Video Type: {VIDEO_TYPE}\")\n",
    "print(f\"Target Duration: {TARGET_DURATION_S} seconds\")\n",
    "print(f\"Analyzer ID: {ANALYZER_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4b8b7",
   "metadata": {},
   "source": [
    "## 1. Generate and Activate Schema\n",
    "*What this does:* Creates a custom Azure AI analyzer that uses OpenAI reasoning to understand your specific video content and identify highlight-worthy moments based on your preferences.\n",
    "\n",
    "The schema generation process uses your VIDEO_TYPE, CLIP_DENSITY, TARGET_DURATION_S, and PERSONALIZATION settings to build an intelligent analyzer that knows exactly what to look for in your video (e.g., goals in soccer, key quotes in presentations, exciting moments in gameplay)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac7221",
   "metadata": {},
   "source": [
    "### Schema Review GUI Instructions\n",
    "âš ï¸ Important:\n",
    "This application uses Tkinter GUI dialogs for human-in-the-loop review.\n",
    "Tkinter requires a graphical display and may not work in headless environments such as GitHub Codespaces, remote servers, Docker containers, or WSL without GUI support. If you encounter `TclError: no display name and no $DISPLAY environment variable`, you can disable human-in-the-loop review by setting `HUMAN_IN_THE_LOOP_REVIEW` to `False`.\n",
    "\n",
    "If you set `HUMAN_IN_THE_LOOP_REVIEW` to `True`, the Schema Review GUI will launch. Here is how to use the Schema Review GUI:\n",
    "\n",
    "1. **Review Schema Fields:**\n",
    "   - Each row displays the field name, its type, and its description.\n",
    "   - Checkboxes next to each field allow you to include or exclude it from the schema.\n",
    "     - Uncheck the box next to any field you want to exclude from the final schema.\n",
    "     - Checked fields will be included; unchecked ones will be removed.\n",
    "2. **Save Button:**\n",
    "   - Click \"Save\" to:\n",
    "     - Save your current field selections to the schema file.\n",
    "     - Refresh the list of fields to reflect your changes.\n",
    "     - Receive a confirmation popup indicating the schema was saved.\n",
    "3. **Add Manual Field:**\n",
    "   - Click \"Add Manual Field\" to open a dialog allowing you to add a new field.\n",
    "     - Enter the field name.\n",
    "     - Select the field type (string, date, time, number, integer).\n",
    "     - Choose the method (classify or generate).\n",
    "     - Enter a description.\n",
    "     - If \"classify\" is selected, you can provide enum options (comma-separated).\n",
    "   - Click \"OK\" to add the field to the schema and update the list.\n",
    "4. **Add GPT Field:**\n",
    "   - Click \"Add GPT Field\" to let GPT suggest a field based on your description.\n",
    "     - Provide a description of the field you want.\n",
    "     - The assistant will generate and add the field to the schema.\n",
    "     - (Requires an OpenAI assistant to be provided.)\n",
    "5. **Done Button:**\n",
    "   - Click \"Done\" when you have finished reviewing.\n",
    "     - The schema will be saved with your final selection.\n",
    "     - The GUI will close.\n",
    "     - A confirmation popup will appear.\n",
    "\n",
    "**Tips:**\n",
    "- Feel free to add, remove, or edit fields as many times as needed before clicking \"Done\".\n",
    "- Use \"Save\" to checkpoint your progress without closing the window.\n",
    "- If you make a mistake, you can re-add fields or adjust your selections before finishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.auto_highlight_utility import activate_schema\n",
    "\n",
    "schema_path = activate_schema(\n",
    "    video_type=VIDEO_TYPE,\n",
    "    analyzer_dir=ANALYZER_DIR,\n",
    "    openai_assistant=openai_assistant,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    clip_density=CLIP_DENSITY,\n",
    "    target_duration_s=TARGET_DURATION_S, \n",
    "    personalization=PERSONALIZATION,\n",
    "    human_in_the_loop_review=HUMAN_IN_THE_LOOP_REVIEW)\n",
    "\n",
    "print(f\"Schema activated: {schema_path}\")\n",
    "\n",
    "# Display schema content\n",
    "with open(schema_path, 'r') as f:\n",
    "    import json\n",
    "    print(json.dumps(json.load(f), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191deeb8",
   "metadata": {},
   "source": [
    "## 2. Analyze Video\n",
    "Now, submit the video to Azure Content Understanding (CU) for analysis using the custom schema you just generated. This step may take some time depending on the length of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07836d",
   "metadata": {},
   "source": [
    "### Create a Custom Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9dc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Use the client to create an analyzer\n",
    "response = cu_client.begin_create_analyzer(ANALYZER_ID, analyzer_template_path=schema_path)\n",
    "result = cu_client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ec78e",
   "metadata": {},
   "source": [
    "### Use the created analyzer to extract video content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147eb3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the video for content analysis\n",
    "response = cu_client.begin_analyze(ANALYZER_ID, file_location=SOURCE_VIDEO_PATH)\n",
    "\n",
    "# Wait for the analysis to complete and retrieve the content analysis result\n",
    "video_cu_result = cu_client.poll_result(\n",
    "    response, timeout_seconds=3600)  # 1 hour timeout for long videos\n",
    "\n",
    "# Print the content analysis result\n",
    "print(f\"Video Content Understanding result:\", video_cu_result)\n",
    "\n",
    "cu_result_filename = f\"{os.path.basename(SOURCE_VIDEO_PATH)}.json\"\n",
    "cu_result_path = os.path.join(OUTPUT_DIR, cu_result_filename)\n",
    "\n",
    "# Save results\n",
    "with open(cu_result_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(video_cu_result, f, indent=2)\n",
    "\n",
    "print(f\"Video analysis complete. Results saved to: {cu_result_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e05b6",
   "metadata": {},
   "source": [
    "### Optional - Delete the analyzer if it is no longer needed\n",
    "Note: In production environments, analyzers are typically kept for reuse rather than deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f765351",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a390ab1",
   "metadata": {},
   "source": [
    "## 3. Parse and Pre-filter Segments\n",
    "\n",
    "Using the output from Azure CU, which describes all the video frames and their content, we apply rule-based filtering to remove less significant frames. This filtering is based on the schema's scoring criteria and creates a simplified list of potential highlight clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854073d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.auto_highlight_utility import get_filtered_segments\n",
    "\n",
    "prefiltered_segments_path = get_filtered_segments(input_path=cu_result_path)\n",
    "print(f\"Segments parsed and pre-filtered. Results saved to: {prefiltered_segments_path}\")\n",
    "\n",
    "# Display the pre-filtered segments\n",
    "with open(prefiltered_segments_path, 'r') as f:\n",
    "    segments_data = json.load(f)\n",
    "    print(json.dumps(segments_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b629e9",
   "metadata": {},
   "source": [
    "## 4. Highlight Edits Filtering\n",
    "Next, we send the parsed segments to OpenAI o3 for advanced reasoning and intelligent timestamp selection. The AI analyzes all potential highlight clips and selects specific timestamps that will create the most compelling custom highlights tailored to your preferences, while meeting the `TARGET_DURATION_S` and arranging them in an optimal narrative order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.utility import get_highlight_plan\n",
    "\n",
    "result = get_highlight_plan(\n",
    "    openai_assistant=openai_assistant,\n",
    "    segments=segments_data,\n",
    "    video_type=VIDEO_TYPE,\n",
    "    target_duration_s=TARGET_DURATION_S,\n",
    "    clip_density=CLIP_DENSITY,\n",
    "    personalization=PERSONALIZATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = result.model_dump()\n",
    "\n",
    "# Save to JSON file\n",
    "highlight_plan_file = \"highlight_plan.json\"\n",
    "highlight_plan_path = os.path.join(OUTPUT_DIR, highlight_plan_file)\n",
    "with open(highlight_plan_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved JSON to {highlight_plan_path}\")\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e50fe",
   "metadata": {},
   "source": [
    "## 5. Stitch Video Clips\n",
    "Based on the highlight plan generated by OpenAI's reasoning, we now stitch together the custom user highlights. This final step takes the intelligently selected timestamps and creates a seamless highlight video tailored to your preferences using FFmpeg for video editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f55a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Stitching using FFmpeg\n",
    "from python.video_modification_utility import stitch_video\n",
    "\n",
    "# Define output path for the highlight video\n",
    "OUTPUT_HIGHLIGHT_PATH = os.path.join(OUTPUT_DIR, \"highlight.mp4\")\n",
    "\n",
    "print(\"ðŸŽ¬ Running video stitching...\")\n",
    "print(f\"ðŸ“„ Input video: {SOURCE_VIDEO_PATH}\")\n",
    "print(f\"ðŸ“„ Highlight plan: {highlight_plan_path}\")\n",
    "print(f\"ðŸ“„ Output video: {OUTPUT_HIGHLIGHT_PATH}\")\n",
    "\n",
    "try:\n",
    "    result = stitch_video(\n",
    "        video_path=SOURCE_VIDEO_PATH,\n",
    "        plan_path=highlight_plan_path,\n",
    "        output_path=OUTPUT_HIGHLIGHT_PATH,\n",
    "        transition=TRANSITION_TYPE,\n",
    "        speed_ramp=SPEED_RAMP,\n",
    "        resolution=RESOLUTION\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        print(f\"âœ… SUCCESS: Highlight video created at {result}\")\n",
    "        \n",
    "        # Check file info\n",
    "        if os.path.exists(result):\n",
    "            size_mb = os.path.getsize(result) / (1024*1024)\n",
    "            print(f\"   ðŸ“ File size: {size_mb:.2f} MB\")\n",
    "            print(f\"   ðŸ“ Full path: {result}\")\n",
    "        print(f\"\\nðŸŽ‰ Highlight generation pipeline completed successfully!\")\n",
    "    else:\n",
    "        print(\"âŒ Video stitching failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ERROR during video stitching: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
