{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11049ef0",
   "metadata": {},
   "source": [
    "# Video Chapters Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beccbe11",
   "metadata": {},
   "source": [
    "Generate video chapters based on Azure Content Understanding and Azure OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44bdf4",
   "metadata": {},
   "source": [
    "\n",
    "## Pre-requisites\n",
    "1. Follow [README](../README.md#configure-azure-ai-service-resource) to create essential resource that will be used in this sample.\n",
    "1. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa60be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefeaab",
   "metadata": {},
   "source": [
    "## Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c69047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "AZURE_AI_SERVICE_ENDPOINT = os.getenv(\"AZURE_AI_SERVICE_ENDPOINT\")\n",
    "AZURE_AI_SERVICE_API_VERSION = os.getenv(\"AZURE_AI_SERVICE_API_VERSION\", \"2025-05-01-preview\")\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-08-01-preview\")\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT_NAME = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe969de",
   "metadata": {},
   "source": [
    "If you haven't done so, please authenticate by running **'az login'** through the terminal. This credentials are used to validate that you have access to the resources you defined above.\n",
    "\n",
    "Make sure you have Azure CLI installed on your system. To install --> curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea18419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Authehticate if you are running this notebook for the first time.\n",
    "\n",
    "import subprocess\n",
    "\n",
    "subprocess.run(\"az login\", shell=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7d414",
   "metadata": {},
   "source": [
    "## File to Analyze\n",
    "\n",
    "Use the following variable to define what file to analyze. For this example, we will be examining a small tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9fb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_LOCATION = \"https://github.com/Azure-Samples/azure-ai-content-understanding-assets/raw/refs/heads/main/videos/learning/learning2.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9abf6",
   "metadata": {},
   "source": [
    "## Create a custom analyzer and submit the video to generate chapters\n",
    "The custom analyzer schema for this notebook is [video_chapters_structured.json](../analyzer_templates/video_chapters_structured.json). This file defines the schema and configuration for a custom video analyzer. It specifies how a video should be segmented into chapters and scenes, including three chapter types: \"Topic Introduction\", \"Details About the Work Done\", and \"Conclusion or Results\". Each segment contains a list of scenes, with each scene described by a short description, start timestamp, and end timestamp. The configuration section controls segmentation behavior and other analysis options, while the fieldSchema section outlines the expected structure of the output, ensuring chapters and scenes are clearly organized and non-overlapping.\n",
    "\n",
    "In this example, we will use the utility class `AzureContentUnderstandingClient` to load the analyzer schema from the template file and submit it to Azure Content Understanding service. Then, we will analyze the video and generate the desired chapter and scene structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e52230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "\n",
    "# add the parent directory to the path to use shared modules\n",
    "parent_dir = Path(Path.cwd()).parent\n",
    "sys.path.append(\n",
    "    str(parent_dir)\n",
    ")\n",
    "from python.content_understanding_client import AzureContentUnderstandingClient\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "# The analyzer template is used to define the schema of the output\n",
    "ANALYZER_TEMPLATE_PATH = \"../analyzer_templates/video_chapters_structured.json\"\n",
    "ANALYZER_ID = \"video_scene_chapter\" + \"_\" + str(uuid.uuid4())  # Unique identifier for the analyzer\n",
    "\n",
    "# Create the Content Understanding (CU) client\n",
    "cu_client = AzureContentUnderstandingClient(\n",
    "    endpoint=AZURE_AI_SERVICE_ENDPOINT,\n",
    "    api_version=AZURE_AI_SERVICE_API_VERSION,\n",
    "    token_provider=token_provider,\n",
    "    x_ms_useragent=\"azure-ai-content-understanding-python/video_chapters_structured\", # This header is used for sample usage telemetry, please comment out this line if you want to opt out.\n",
    ")\n",
    "\n",
    "# Use the client to create an analyzer\n",
    "response = cu_client.begin_create_analyzer(\n",
    "    ANALYZER_ID, analyzer_template_path=ANALYZER_TEMPLATE_PATH)\n",
    "result = cu_client.poll_result(response)\n",
    "\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85cf38",
   "metadata": {},
   "source": [
    "### Use the created analyzer to extract video content\n",
    "It might take some time depending on the video length. Try with short videos to get results faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the video for content analysis\n",
    "response = cu_client.begin_analyze(ANALYZER_ID, file_location=VIDEO_LOCATION)\n",
    "\n",
    "# Wait for the analysis to complete and get the content analysis result\n",
    "video_cu_result = cu_client.poll_result(\n",
    "    response, timeout_seconds=3600)  # 1 hour timeout for long videos\n",
    "\n",
    "# Print the content analysis result\n",
    "print(f\"Video Content Understanding result: \", video_cu_result)\n",
    "\n",
    "# Optional - Delete the analyzer if it is no longer needed\n",
    "cu_client.delete_analyzer(ANALYZER_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d1342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from python.chapters_utility import ChaptersFormatter\n",
    "\n",
    "full_html = ChaptersFormatter.format_chapters_output(VIDEO_LOCATION, video_cu_result)\n",
    "display(HTML(full_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126b263",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
